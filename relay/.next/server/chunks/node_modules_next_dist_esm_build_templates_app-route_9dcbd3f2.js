module.exports=[46920,e=>{"use strict";var t=e.i(47909),o=e.i(74017),a=e.i(96250),r=e.i(84243),n=e.i(61916),s=e.i(14444),i=e.i(37092),l=e.i(69741),c=e.i(16795),p=e.i(87718),d=e.i(95169),u=e.i(47587),m=e.i(66012),g=e.i(70101),f=e.i(26937),h=e.i(10372),v=e.i(93695);e.i(52474);var w=e.i(220);async function S(e){return new Response(JSON.stringify({error:"WebSocket endpoint not implemented",message:"This endpoint requires a separate WebSocket server. See code comments for implementation options.",documentation:"This file serves as documentation for the required WebSocket implementation.",recommendedApproach:"Deploy separate Node.js WebSocket server with ws + openai libraries"}),{status:501,headers:{"Content-Type":"application/json"}})}let y=`
const WebSocket = require('ws');
const OpenAI = require('openai');
const admin = require('firebase-admin');

// Initialize Firebase Admin
admin.initializeApp({
  credential: admin.credential.cert({
    projectId: process.env.FIREBASE_PROJECT_ID,
    clientEmail: process.env.FIREBASE_CLIENT_EMAIL,
    privateKey: process.env.FIREBASE_PRIVATE_KEY.replace(/\\\\n/g, '\\n'),
  }),
});

const db = admin.firestore();
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Create WebSocket server
const wss = new WebSocket.Server({ port: process.env.WS_PORT || 8080 });

console.log('ðŸŽ™ï¸  WebSocket server running on port', process.env.WS_PORT || 8080);

wss.on('connection', async (twilioWs, req) => {
  console.log('ðŸ“ž Twilio connected');

  // Parse context from query parameter
  const url = new URL(req.url, \`http://\${req.headers.host}\`);
  const contextParam = url.searchParams.get('context');
  
  if (!contextParam) {
    console.error('âŒ Missing context parameter');
    twilioWs.close();
    return;
  }

  const context = JSON.parse(decodeURIComponent(contextParam));
  
  let streamSid = null;
  let callSid = null;
  let openaiWs = null;
  let transcript = [];
  let audioBuffer = [];

  try {
    // Connect to OpenAI Realtime API
    openaiWs = new WebSocket('wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17', {
      headers: {
        'Authorization': \`Bearer \${process.env.OPENAI_API_KEY}\`,
        'OpenAI-Beta': 'realtime=v1',
      },
    });

    openaiWs.on('open', () => {
      console.log('ðŸ¤– OpenAI Realtime API connected');

      // Configure OpenAI session
      openaiWs.send(JSON.stringify({
        type: 'session.update',
        session: {
          modalities: ['text', 'audio'],
          instructions: context.aiInstructions,
          voice: 'alloy', // Professional female voice
          input_audio_format: 'g711_ulaw', // Twilio format
          output_audio_format: 'g711_ulaw',
          input_audio_transcription: {
            model: 'whisper-1', // Transcribe user input
          },
          turn_detection: {
            type: 'server_vad', // Voice Activity Detection by OpenAI
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 500,
          },
          temperature: 0.7, // Slightly creative but consistent
          max_response_output_tokens: 4096,
        },
      }));

      // Send initial context to AI
      openaiWs.send(JSON.stringify({
        type: 'conversation.item.create',
        item: {
          type: 'message',
          role: 'system',
          content: [{
            type: 'input_text',
            text: \`You are calling \${context.recipientName} about invoice \${context.invoiceReference} for \xa3\${context.amount}, which is \${context.daysPastDue} days overdue. Business name: \${context.businessName}.\`,
          }],
        },
      }));

      openaiWs.send(JSON.stringify({ type: 'response.create' }));
    });

    // Handle messages from OpenAI
    openaiWs.on('message', (data) => {
      try {
        const message = JSON.parse(data.toString());

        // Forward audio from OpenAI to Twilio
        if (message.type === 'response.audio.delta' && message.delta) {
          if (streamSid) {
            twilioWs.send(JSON.stringify({
              event: 'media',
              streamSid,
              media: {
                payload: message.delta, // Base64 encoded Î¼-law audio
              },
            }));
          }
        }

        // Capture transcript from OpenAI
        if (message.type === 'response.audio_transcript.delta' && message.delta) {
          transcript.push({
            role: 'assistant',
            text: message.delta,
            timestamp: new Date().toISOString(),
          });
        }

        if (message.type === 'conversation.item.input_audio_transcription.completed') {
          transcript.push({
            role: 'user',
            text: message.transcript,
            timestamp: new Date().toISOString(),
          });
        }

        // Handle errors from OpenAI
        if (message.type === 'error') {
          console.error('âŒ OpenAI error:', message.error);
        }

        // Log function calls if AI wants to send payment link
        if (message.type === 'response.function_call_arguments.done') {
          console.log('ðŸ”§ Function call:', message.name, message.arguments);
          // TODO: Implement function to send SMS payment link
        }

      } catch (err) {
        console.error('âŒ Error processing OpenAI message:', err);
      }
    });

    openaiWs.on('error', (error) => {
      console.error('âŒ OpenAI WebSocket error:', error);
    });

    openaiWs.on('close', () => {
      console.log('ðŸ¤– OpenAI connection closed');
    });

    // Handle messages from Twilio
    twilioWs.on('message', (data) => {
      try {
        const message = JSON.parse(data.toString());

        switch (message.event) {
          case 'start':
            streamSid = message.start.streamSid;
            callSid = message.start.callSid;
            console.log(\`ðŸ“ž Stream started: \${streamSid}\`);
            break;

          case 'media':
            // Forward audio from Twilio to OpenAI
            if (openaiWs && openaiWs.readyState === WebSocket.OPEN) {
              openaiWs.send(JSON.stringify({
                type: 'input_audio_buffer.append',
                audio: message.media.payload, // Base64 encoded Î¼-law audio
              }));
            }
            break;

          case 'stop':
            console.log('ðŸ“ž Stream ended');
            
            // Save transcript and call outcome to Firestore
            saveCallOutcome(context, callSid, transcript).catch(console.error);
            
            if (openaiWs) {
              openaiWs.close();
            }
            break;
        }
      } catch (err) {
        console.error('âŒ Error processing Twilio message:', err);
      }
    });

    twilioWs.on('error', (error) => {
      console.error('âŒ Twilio WebSocket error:', error);
    });

    twilioWs.on('close', () => {
      console.log('ðŸ“ž Twilio disconnected');
      if (openaiWs) {
        openaiWs.close();
      }
    });

  } catch (error) {
    console.error('âŒ Connection error:', error);
    twilioWs.close();
    if (openaiWs) {
      openaiWs.close();
    }
  }
});

/**
 * Save call transcript and outcome to Firestore
 */
async function saveCallOutcome(context, callSid, transcript) {
  try {
    console.log('ðŸ’¾ Saving call outcome to Firestore');

    // Find collection attempt by callSid
    const attemptsSnapshot = await db
      .collection('collection_attempts')
      .where('callSID', '==', callSid)
      .limit(1)
      .get();

    if (attemptsSnapshot.empty) {
      console.error('âŒ Collection attempt not found for callSid:', callSid);
      return;
    }

    const attemptDoc = attemptsSnapshot.docs[0];
    
    // Combine transcript into text
    const fullTranscript = transcript
      .map(t => \`[\${t.role.toUpperCase()}] \${t.text}\`)
      .join('\\n');

    // Use OpenAI to analyze transcript and extract outcome
    const analysis = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: \`Analyze this debt collection call transcript and extract structured data.
Return JSON with:
- outcome: 'paid' | 'promise' | 'partial' | 'refused' | 'no_answer' | 'voicemail' | 'dispute' | 'financial_hardship' | 'error'
- summary: 2-3 sentence summary of call
- nextAction: 'accept_partial' | 'schedule_followup' | 'escalate' | 'pause' | 'complete' | 'agency_handoff'
- scheduledFollowupDate: if debtor promised to pay by date (YYYY-MM-DD)
- partialAmountAgreed: if partial payment agreed (number)
- clientProposedDate: if client suggested payment date (YYYY-MM-DD)
- disputeDetails: if debt was disputed, what are the details
- paymentMethod: 'sms_link' | 'ivr' | 'bank_transfer'
- consentGiven: did caller consent to recording (boolean)\`,
        },
        {
          role: 'user',
          content: fullTranscript,
        },
      ],
      response_format: { type: 'json_object' },
    });

    const outcome = JSON.parse(analysis.choices[0].message.content);

    // Update Firestore
    await attemptDoc.ref.update({
      callTranscript: fullTranscript,
      callNotes: outcome.summary,
      callOutcome: outcome.outcome,
      nextAction: outcome.nextAction,
      clientProposedDate: outcome.clientProposedDate || null,
      partialAmountAgreed: outcome.partialAmountAgreed || null,
      paymentMethod: outcome.paymentMethod || null,
      consentGiven: outcome.consentGiven !== false, // Default to true unless explicitly refused
      callEndedAt: admin.firestore.FieldValue.serverTimestamp(),
      result: outcome.outcome === 'paid' || outcome.outcome === 'promise' || outcome.outcome === 'partial' ? 'success' : 'failed',
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
    });

    console.log('âœ… Call outcome saved:', outcome.outcome);

  } catch (error) {
    console.error('âŒ Error saving call outcome:', error);
  }
}

// Handle graceful shutdown
process.on('SIGTERM', () => {
  console.log('âš ï¸  SIGTERM received, closing WebSocket server');
  wss.close(() => {
    console.log('ðŸ‘‹ WebSocket server closed');
    process.exit(0);
  });
});
`;e.s(["GET",()=>S,"REFERENCE_IMPLEMENTATION",()=>y],86480);var A=e.i(86480);let R=new t.AppRouteRouteModule({definition:{kind:o.RouteKind.APP_ROUTE,page:"/api/webhooks/twilio/voice-stream/route",pathname:"/api/webhooks/twilio/voice-stream",filename:"route",bundlePath:""},distDir:".next",relativeProjectDir:"",resolvedPagePath:"[project]/app/api/webhooks/twilio/voice-stream/route.ts",nextConfigOutput:"",userland:A}),{workAsyncStorage:E,workUnitAsyncStorage:_,serverHooks:x}=R;function b(){return(0,a.patchFetch)({workAsyncStorage:E,workUnitAsyncStorage:_})}async function O(e,t,a){R.isDev&&(0,r.addRequestMeta)(e,"devRequestTimingInternalsEnd",process.hrtime.bigint());let S="/api/webhooks/twilio/voice-stream/route";S=S.replace(/\/index$/,"")||"/";let y=await R.prepare(e,t,{srcPage:S,multiZoneDraftMode:!1});if(!y)return t.statusCode=400,t.end("Bad Request"),null==a.waitUntil||a.waitUntil.call(a,Promise.resolve()),null;let{buildId:A,params:E,nextConfig:_,parsedUrl:x,isDraftMode:b,prerenderManifest:O,routerServerContext:I,isOnDemandRevalidate:T,revalidateOnlyGenerated:C,resolvedPathname:N,clientReferenceManifest:P,serverActionsManifest:W}=y,k=(0,l.normalizeAppPath)(S),D=!!(O.dynamicRoutes[k]||O.routes[N]),M=async()=>((null==I?void 0:I.render404)?await I.render404(e,t,x,!1):t.end("This page could not be found"),null);if(D&&!b){let e=!!O.routes[N],t=O.dynamicRoutes[k];if(t&&!1===t.fallback&&!e){if(_.experimental.adapterPath)return await M();throw new v.NoFallbackError}}let F=null;!D||R.isDev||b||(F="/index"===(F=N)?"/":F);let q=!0===R.isDev||!D,H=D&&!q;W&&P&&(0,s.setReferenceManifestsSingleton)({page:S,clientReferenceManifest:P,serverActionsManifest:W,serverModuleMap:(0,i.createServerModuleMap)({serverActionsManifest:W})});let U=e.method||"GET",$=(0,n.getTracer)(),B=$.getActiveScopeSpan(),j={params:E,prerenderManifest:O,renderOpts:{experimental:{authInterrupts:!!_.experimental.authInterrupts},cacheComponents:!!_.cacheComponents,supportsDynamicResponse:q,incrementalCache:(0,r.getRequestMeta)(e,"incrementalCache"),cacheLifeProfiles:_.cacheLife,waitUntil:a.waitUntil,onClose:e=>{t.on("close",e)},onAfterTaskError:void 0,onInstrumentationRequestError:(t,o,a)=>R.onRequestError(e,t,a,I)},sharedContext:{buildId:A}},K=new c.NodeNextRequest(e),J=new c.NodeNextResponse(t),Y=p.NextRequestAdapter.fromNodeNextRequest(K,(0,p.signalFromNodeResponse)(t));try{let s=async e=>R.handle(Y,j).finally(()=>{if(!e)return;e.setAttributes({"http.status_code":t.statusCode,"next.rsc":!1});let o=$.getRootSpanAttributes();if(!o)return;if(o.get("next.span_type")!==d.BaseServerSpan.handleRequest)return void console.warn(`Unexpected root span type '${o.get("next.span_type")}'. Please report this Next.js issue https://github.com/vercel/next.js`);let a=o.get("next.route");if(a){let t=`${U} ${a}`;e.setAttributes({"next.route":a,"http.route":a,"next.span_name":t}),e.updateName(t)}else e.updateName(`${U} ${S}`)}),i=!!(0,r.getRequestMeta)(e,"minimalMode"),l=async r=>{var n,l;let c=async({previousCacheEntry:o})=>{try{if(!i&&T&&C&&!o)return t.statusCode=404,t.setHeader("x-nextjs-cache","REVALIDATED"),t.end("This page could not be found"),null;let n=await s(r);e.fetchMetrics=j.renderOpts.fetchMetrics;let l=j.renderOpts.pendingWaitUntil;l&&a.waitUntil&&(a.waitUntil(l),l=void 0);let c=j.renderOpts.collectedTags;if(!D)return await (0,m.sendResponse)(K,J,n,j.renderOpts.pendingWaitUntil),null;{let e=await n.blob(),t=(0,g.toNodeOutgoingHttpHeaders)(n.headers);c&&(t[h.NEXT_CACHE_TAGS_HEADER]=c),!t["content-type"]&&e.type&&(t["content-type"]=e.type);let o=void 0!==j.renderOpts.collectedRevalidate&&!(j.renderOpts.collectedRevalidate>=h.INFINITE_CACHE)&&j.renderOpts.collectedRevalidate,a=void 0===j.renderOpts.collectedExpire||j.renderOpts.collectedExpire>=h.INFINITE_CACHE?void 0:j.renderOpts.collectedExpire;return{value:{kind:w.CachedRouteKind.APP_ROUTE,status:n.status,body:Buffer.from(await e.arrayBuffer()),headers:t},cacheControl:{revalidate:o,expire:a}}}}catch(t){throw(null==o?void 0:o.isStale)&&await R.onRequestError(e,t,{routerKind:"App Router",routePath:S,routeType:"route",revalidateReason:(0,u.getRevalidateReason)({isStaticGeneration:H,isOnDemandRevalidate:T})},I),t}},p=await R.handleResponse({req:e,nextConfig:_,cacheKey:F,routeKind:o.RouteKind.APP_ROUTE,isFallback:!1,prerenderManifest:O,isRoutePPREnabled:!1,isOnDemandRevalidate:T,revalidateOnlyGenerated:C,responseGenerator:c,waitUntil:a.waitUntil,isMinimalMode:i});if(!D)return null;if((null==p||null==(n=p.value)?void 0:n.kind)!==w.CachedRouteKind.APP_ROUTE)throw Object.defineProperty(Error(`Invariant: app-route received invalid cache entry ${null==p||null==(l=p.value)?void 0:l.kind}`),"__NEXT_ERROR_CODE",{value:"E701",enumerable:!1,configurable:!0});i||t.setHeader("x-nextjs-cache",T?"REVALIDATED":p.isMiss?"MISS":p.isStale?"STALE":"HIT"),b&&t.setHeader("Cache-Control","private, no-cache, no-store, max-age=0, must-revalidate");let d=(0,g.fromNodeOutgoingHttpHeaders)(p.value.headers);return i&&D||d.delete(h.NEXT_CACHE_TAGS_HEADER),!p.cacheControl||t.getHeader("Cache-Control")||d.get("Cache-Control")||d.set("Cache-Control",(0,f.getCacheControlHeader)(p.cacheControl)),await (0,m.sendResponse)(K,J,new Response(p.value.body,{headers:d,status:p.value.status||200})),null};B?await l(B):await $.withPropagatedContext(e.headers,()=>$.trace(d.BaseServerSpan.handleRequest,{spanName:`${U} ${S}`,kind:n.SpanKind.SERVER,attributes:{"http.method":U,"http.target":e.url}},l))}catch(t){if(t instanceof v.NoFallbackError||await R.onRequestError(e,t,{routerKind:"App Router",routePath:k,routeType:"route",revalidateReason:(0,u.getRevalidateReason)({isStaticGeneration:H,isOnDemandRevalidate:T})}),D)throw t;return await (0,m.sendResponse)(K,J,new Response(null,{status:500})),null}}e.s(["handler",()=>O,"patchFetch",()=>b,"routeModule",()=>R,"serverHooks",()=>x,"workAsyncStorage",()=>E,"workUnitAsyncStorage",()=>_],46920)}];

//# sourceMappingURL=node_modules_next_dist_esm_build_templates_app-route_9dcbd3f2.js.map